---
title: "WdraÅ¼anie testÃ³w w AI: fundament dobrej aplikacji"
date: 2024-10-08T23:00:00+02:00
lastmod: 2024-10-08T23:00:00+02:00
summary: "Jak mierzyÄ‡ skutecznoÅ›Ä‡ aplikacji? Dlaczego? Na co uwaÅ¼aÄ‡? - artykuÅ‚ odpowie na te pytania i umoÅ¼liwi Ci tworzenie lepszych aplikacji opartych o AI!"
thumbnail: "thumbnail.jpg"
tags: ["ulepsz swoje AI", "ai", "python", "llm", "rag"]
---

ArtykuÅ‚ jest skierowany dla kaÅ¼dej osoby pracujÄ…cej z AI. JeÅ›li dopiero zaczynasz, polecam wytÅ‚umaczenie [co to LLM i RAG](/blog/retrieval-augmented-generation) ğŸš€

## Po co w ogÃ³le mierzyÄ‡ skutecznoÅ›Ä‡?

WyobraÅºmy sobie nastÄ™pujÄ…cÄ… sytuacjÄ™: 

Odpowiadasz razem z Twoim zespoÅ‚em za rozwÃ³j najnowszej aplikacji AI: generator poematÃ³w o kotach.

Spokojny piÄ…tkowy poranek. TwÃ³j *<klient / uÅ¼ytkownik / tester>* zgÅ‚asza krytyczny bÅ‚Ä…d: aplikacja nagle napisaÅ‚a poemat o psie! DostaÅ‚eÅ› zadanie, aby to naprawiÄ‡, wiÄ™c dostosowujesz mechanizm pod znaleziony przypadek. Ponownie testujesz zaÅ‚Ä…czone pytanie i... dziaÅ‚a, wiÄ™c wdraÅ¼asz poprawkÄ™ na produkcjÄ™ :)

No wÅ‚aÅ›nie - czy na pewno dziaÅ‚a? Nie masz zestawu testÃ³w, wiÄ™c:
- nie wiesz, Å¼e naprawa 1 przypadku spowodowaÅ‚a wygenerowanie bÅ‚Ä™dnej odpowiedzi w 10 innych przypadkach;
- po zmianach Twoja aplikacja czasami zaczÄ™Å‚a generowaÄ‡ poematy po chiÅ„sku - nie wiesz dlaczego, ale musisz przygotowaÄ‡ kolejnÄ… Å‚atkÄ™;
- wczeÅ›niej zgÅ‚oszony bÅ‚Ä…d, ktÃ³ry naprawiÅ‚eÅ› miesiÄ…c temu, nagle zaczÄ…Å‚ ponownie wystÄ™powaÄ‡ w systemie;
- nie jesteÅ› w stanie stwierdziÄ‡, czy Twoja aplikacja siÄ™ w ogÃ³le poprawia - w koÅ„cu caÅ‚y czas pojawiajÄ… siÄ™ nowe bÅ‚Ä™dy! Jak udowodnisz, Å¼e zmiany faktycznie rozwijajÄ… aplikacjÄ™?

I tutaj wchodzÄ… **testy, ktÃ³re rozwiÄ…Å¼Ä… Twoje problemy**! Nie tylko generujÄ…c poematy o kotach, ale w szczegÃ³lnoÅ›ci w realnych aplikacjach: chatbotach, systemach agentowych, narzÄ™dziach do przetwarzania dokumentÃ³w, etc.

### Testy w aplikacjach AI sÄ… konieczne

**Wprowadzenie testÃ³w pomoÅ¼e Wam ustaliÄ‡**, Å¼e zmiana X polepszyÅ‚a skutecznoÅ›Ä‡ aplikacji z 80% do 85%, a kolejna zmiana Y pogorszyÅ‚a skutecznoÅ›Ä‡ z 85% do 73%. DziÄ™ki temu:
- bÄ™dziesz w stanie jasno ustaliÄ‡, czy skutecznoÅ›Ä‡ aplikacji siÄ™ polepsza, lub czy np. ma sÅ‚abe punkty;
- zapobiegniesz nieÅ›wiadomego wprowadzenia regresji, czyli pogorszenia jakoÅ›ci odpowiedzi lub zwiÄ™kszenia czÄ™stotliwoÅ›ci bÅ‚Ä™dÃ³w;
- ponadto: tworzÄ…c testy, bardzo szybko wyklarujÄ… siÄ™ wymagania Twojej aplikacji oraz co powinno byÄ‡ Twoim priorytetem

**UwaÅ¼am, Å¼e w aplikacjach opartych o AI testy to koniecznoÅ›Ä‡**! Nie moÅ¼esz ich caÅ‚kowicie pominÄ…Ä‡, np. poprzez prÃ³bÄ™ zminimalizowania kosztÃ³w. To tylko zadziaÅ‚a, jeÅ›li Twoja aplikacja nie musi byÄ‡ dobra, lub przewidujesz, Å¼e nie trafi nigdzie na produkcjÄ™ - wtedy droga wolna ;) 
### MÃ³j CRUD jest w 100% poprawny i nie ma Å¼adnych bÅ‚Ä™dÃ³w. Dlaczego tak nie mogÄ™ zrobiÄ‡ z AI?

Tradycyjne aplikacje (bez AI) opierajÄ… siÄ™ o logikÄ™, ktÃ³rÄ… zawsze jesteÅ›my w stanie dostosowaÄ‡ pod nasze wymagania biznesowe. JeÅ›li nie mamy bÅ‚Ä™dÃ³w w implementacji, nasza aplikacja zawsze bÄ™dzie robiÅ‚a dokÅ‚adnie to, czego od niej oczekujemy.

Nie moÅ¼emy zagwarantowaÄ‡ takiej 100% skutecznoÅ›ci w aplikacjach opartych o AI. Gdy nasza aplikacja odpowiada poprawnie na 99% zapytaÅ„ uÅ¼ytkownikÃ³w, to nadal jesteÅ›my w stanie znaleÅºÄ‡ brakujÄ…cy odsetek 1%, w ktÃ³rym pojawiÅ‚y siÄ™ bÅ‚Ä™dy. A testy manualne wykaÅ¼Ä… jedynie ten 1% bÅ‚Ä™dÃ³w - bez zwrÃ³cenia uwagi na skalÄ™ problemu :)

{{< notice-feather-icons tip >}}
**Dlaczego nie moÅ¼emy zagwarantowaÄ‡ 100% skutecznoÅ›ci?**

Pod spodem modelu jÄ™zykowego, jak np. ChatGPT, nie mamy serii instrukcji warunkowych (tzw. ifÃ³w) okreÅ›lajÄ…cych formalnÄ… logikÄ™, tylko funkcjÄ™ matematycznÄ… (sieÄ‡ neuronowÄ…), ktÃ³ra zawiera kilkaset tysiÄ™cy miliardÃ³w parametrÃ³w! Te liczby odpowiadajÄ… za prawdopodobieÅ„stwa wygenerowanie tekstu. 
 
Sieci neuronowe nie mogÄ… byÄ‡ przez to w 100% poprawne. Zawsze znajdzie siÄ™ jakieÅ› zachowanie, ktÃ³re wybiega poza moÅ¼liwoÅ›ci przewidywania tekstu. StÄ…d musimy potraktowaÄ‡ bÅ‚Ä™dy jako coÅ›, czego nie moÅ¼emy w peÅ‚ni wyeliminowaÄ‡, ale dÄ…Å¼ymy do zminimalizowania.

To samo tyczy siÄ™ innych modeli ML - na przykÅ‚ad klasyfikatorÃ³w obrazÃ³w.
{{</ notice-feather-icons >}}

## Jak ewaluowaÄ‡ jakoÅ›Ä‡ odpowiedzi? 

ZacznÄ™ od abstrakcyjnego frameworka, bo niezaleÅ¼nie od Twojej aplikacji:
1. **StwÃ³rz zbiÃ³r przypadkÃ³w testowych** i oczekiwanych kryteriÃ³w odpowiedzi. Kryteria powinny byÄ‡ moÅ¼liwie jasne w interpretacji, np. "odpowiedÅº X zawiera informacje o Y", albo "zdjÄ™cie K zostaÅ‚o sklasyfikowane jako kot";
2. Zmierz, jak Twoja aplikacja radzi sobie z kaÅ¼dym przypadkiem i **oblicz jaki % przypadkÃ³w byÅ‚ poprawny**;
3. **Powtarzaj krok drugi** po kaÅ¼dych zmianach :)

Brzmi czasochÅ‚onnie? OtÃ³Å¼ nie do koÅ„ca: 
- zbiÃ³r testÃ³w musisz stworzyÄ‡ tylko raz;
- odpowiedzi mogÄ… byÄ‡ automatycznie ewaluowane za pomocÄ… LLM;
- istniejÄ… do tego gotowe narzÄ™dzia!

PrzedstawiÄ™ realny przykÅ‚ad na przykÅ‚adzie chatbota:

### Automatyczna ewaluacja za pomocÄ… LLMÃ³w

Przyjmijmy, Å¼e pracujemy nad chatbotem dla producenta lodÃ³wek - stwÃ³rzmy 1. przypadek testowy:
- Pytanie: "Jak siÄ™ z Wami skontaktowaÄ‡?"
- Kryteria oceny:
	- OdpowiedÅº musi zawieraÄ‡ adres "Szczebrzeszyn, ul. GÃ³rna 123"
	- OdpowiedÅº musi zawieraÄ‡ nr telefonu "+48 123 456 789"
	- OdpowiedÅº powinna byÄ‡ napisana profesjonalnym tonem

Gdy chatbot wygeneruje odpowiedÅº - np. "No elo mordeczko, moÅ¼esz skontaktowaÄ‡ siÄ™ z nami pod nr +48 123 456 789, albo wpaÅ›Ä‡ do nas w Szczebrzeszynie na ul. GÃ³rnej 123. Pozdro!" - to moÅ¼emy poinstruktowaÄ‡ LLMa specjalnymi promptami: 

> - âœ… Odpowiedz TAK, jeÅ›li wiadomoÅ›Ä‡ "No elo mordeczko..." zawiera adres "Szczebrzeszyn ..."
> - âœ… Odpowiedz TAK, jeÅ›li wiadomoÅ›Ä‡ "..." zawiera nr telefonu "..."
> - âŒ Odpowiedz TAK, jeÅ›li wiadomoÅ›Ä‡ jest napisana profesjonalnym tonem.

Efekt? Model ustali, Å¼e wiadomoÅ›Ä‡ nie speÅ‚nia wszystkich kryteriÃ³w. Dowiemy siÄ™, co byÅ‚o w niej nie tak. Zajmie to sekundy i bÄ™dzie kosztowaÅ‚o uÅ‚amki centÃ³w :)  

### Promptfoo - narzÄ™dzie do pomiaru skutecznoÅ›ci

Prawdopodobnie nie bÄ™dziesz nawet musiaÅ‚ programowaÄ‡ takiego frameworka! PrzedstawiÄ™ Ci sprawdzone narzÄ™dzie open-source, ktÃ³re moÅ¼e okazaÄ‡ siÄ™ dostateczne:

[Promptfoo](https://www.promptfoo.dev/) to moim zdaniem najlepsze narzÄ™dzie do testowania promptÃ³w oraz aplikacji RAG. Jest bezpÅ‚atne, stale rozwijane oraz stara siÄ™ byÄ‡ moÅ¼liwie uniwersalne. Post nie jest sponsorowany ;)

ZachÄ™cam do zapoznania siÄ™ z [README na Githubie](https://github.com/promptfoo/promptfoo). PoniÅ¼ej zamieszcza przykÅ‚adowÄ… konfiguracjÄ™, ktÃ³ra:
- porÃ³wnuje 2 prompty do tÅ‚umaczenia wiadomoÅ›ci
- porÃ³wnuje 2 rÃ³Å¼nych providerÃ³w LLM (gpt-4o i lokalnÄ… llamÄ™ 3)
- dla 4 wariantÃ³w (2 rÃ³Å¼ne prompty razy 2 rÃ³Å¼nych providerÃ³w) wykona 2 przypadki testowe, z czego:
	- pierwszy przetestuje tÅ‚umaczenie na jÄ™zyk francuski, i sprawdzi czy output ma mniej niÅ¼ 100 znakÃ³w
	- drugi przetestuje tÅ‚umaczenie na jÄ™zyk niemiecki, sprawdzi podobieÅ„stwo outputu na podstawie embeddingÃ³w (cosine similarity) oraz zapewni, Å¼e model nie opowiada o sobie jako o modelu jÄ™zykowym

```yaml
prompts:
  - file://prompt1.txt
  - file://prompt2.txt
providers:
  - openai:gpt-4o-mini
  - ollama:llama3.1:70b
tests:
  - description: 'Test translation to French'
    vars:
      language: French
      input: Hello world
    assert:
      - type: contains-json
      - type: javascript
        value: output.length < 100

  - description: 'Test translation to German'
    vars:
      language: German
      input: How's it going?
    assert:
      - type: llm-rubric
        value: does not describe self as an AI, model, or chatbot
      - type: similar
        value: was geht
        threshold: 0.6 # cosine similarity
```

MoÅ¼liwoÅ›ci sÄ… niezwykle szerokie, wiÄ™c bez problemu dostosujesz konfiguracjÄ™ pod swoje potrzeby. Output moÅ¼esz podejrzeÄ‡ w formie GUI lub odczytaÄ‡ z CLI:

{{< gallerystart >}}
{{< img src="promptfoo.png" caption="Promptfoo - GUI" >}}
{{< img src="promptfoo cli.png" caption="Promptfoo - CLI" >}}
{{< galleryend >}}

NarzÄ™dziem moÅ¼esz sterowaÄ‡ z poziomu skryptÃ³w, np. zintegrowaÄ‡ z CI/CD. Promptfoo umoÅ¼liwia ustalenie [dowolnej aplikacji jako providera](https://www.promptfoo.dev/docs/providers/webhook/), czyli nie przetestujesz tym wyÅ‚Ä…cznie LLMa/prompta, ale rÃ³wnieÅ¼ customowÄ… aplikacjÄ™.

## Tworzenie dobrych testÃ³w

Zostawiam Ci kilka praktycznych porad - bo diabeÅ‚ tkwi w szczegÃ³Å‚ach:
### StwÃ³rz benchmark juÅ¼ na samym poczÄ…tku pracy 

Zacznij od ustalenia: na jakie pytania powinna odpowiadaÄ‡ Twoja aplikacja? Jakie powinny byÄ‡ ich oczekiwane odpowiedzi? Na co zwracaÄ‡ uwagÄ™? 

MajÄ…c gotowÄ… listÄ™, moÅ¼esz stworzyÄ‡ prototyp aplikacji. Stworzenie dziaÅ‚ajÄ…cego MVP da Tobie punkt odniesienia, ktÃ³ry nastÄ™pnie bÄ™dziesz mÃ³gÅ‚ stopniowo ulepszaÄ‡. Nie bÄ™dziesz musiaÅ‚ polegaÄ‡ na subiektywnych odczuciach, ale na zbiorze metryk skutecznoÅ›ci: 50% trafnoÅ›ci, 60% trafnoÅ›ci, ... 

MoÅ¼e okaÅ¼e siÄ™, Å¼e na Twoje potrzeby wczesny prototyp bÄ™dzie wystarczajÄ…co dobry? Albo w drugÄ… stronÄ™ - niezwykle niska skutecznoÅ›Ä‡ zasygnalizuje Ci, Å¼e pomysÅ‚ jest znacznie ciÄ™Å¼szy niÅ¼ myÅ›laÅ‚eÅ›.
### Nie twÃ³rz testÃ³w pod zebrane dane; twÃ³rz testy pod uÅ¼ytkownika

Åatwo wpaÅ›Ä‡ w puÅ‚apkÄ™ realizacji testÃ³w na podstawie aktualnie posiadanych danych.

Czujesz, Å¼e pytanie bÄ™dzie na tyle ciÄ™Å¼kie, Å¼e system sobie nie poradzi, bo np. wymaga wiedzy z dwÃ³ch rÃ³Å¼nych ÅºrÃ³deÅ‚ systemu, wiÄ™c nie dodajesz pytania do zbioru testowego? Albo przeglÄ…dasz bazÄ™ danych, tworzÄ…c pytania bezpoÅ›rednio w oparciu o sÅ‚owa kluczowe z oryginalnych tekstÃ³w?

To znak dla Ciebie, aby bardziej realnie przetestowaÄ‡ swÃ³j system ;) UÅ¼ytkownik wykorzystuje synonimy, czÄ™sto gubi interpunkcjÄ™ czy duÅ¼e znaki. MoÅ¼e nie uÅ¼ywaÄ‡ terminÃ³w, ktÃ³re Ty znasz. MoÅ¼e wykorzystuje prostszy jÄ™zyk, moÅ¼e fachowÄ… terminologiÄ™, a moÅ¼e korzysta z akronimÃ³w i skrÃ³towcÃ³w?

PamiÄ™taj, Å¼e celem testÃ³w jest zadowolenie uÅ¼ytkownika koÅ„cowego, a nie sztuczne podbicie metryczki.
### Zadbaj o zrÃ³Å¼nicowane pytania testowe

Nie umieszczaj w testach tylko i wyÅ‚Ä…cznie ciÄ™Å¼kich pytaÅ„ - umieszczenie pozornie prostych pytaÅ„ pozwoli wyÅ‚apaÄ‡ powaÅ¼ne regresje, na ktÃ³re na pewno trafiÅ‚by uÅ¼ytkownik Waszej aplikacji. 

Na przykÅ‚ad: jeÅ›li tworzysz aplikacjÄ™ opartÄ… o RAG, ktÃ³ra moÅ¼e nie dziaÅ‚aÄ‡ prawidÅ‚owo przez rÃ³Å¼ne przyczyny, wiÄ™c Twoje pytania testowe powinny zabezpieczaÄ‡ przed moÅ¼liwie najszerszym zakresem poraÅ¼ek:
- sprawdÅº, czy LLM nie halucynuje przy pytaniach spoza zakresu wiedzy;
- zobacz, czy retrieval dziaÅ‚a poprawnie przy przypadkach brzegowych (synonimy, akronimy, inna terminologia, niekompletna polszczyzna);
- czy moÅ¼na wymusiÄ‡ niestosownÄ… odpowiedÅº?
- czy moÅ¼na wymusiÄ‡, aby LLM zwrÃ³ciÅ‚ prompt systemowy? czy moÅ¼na sprawiÄ‡, aby zignorowaÅ‚ instrukcje i zrobiÅ‚ coÅ› innego? 

JeÅ›li TwÃ³j system jest wdroÅ¼ony na produkcjÄ™, zalecam **monitorowaÄ‡ zapytania uÅ¼ytkownikÃ³w koÅ„cowych** (o ile to moÅ¼liwe) - nic nie stanowi lepszych testÃ³w :) 
### Overfitting

Tak jak model ML moÅ¼ecie overfittowaÄ‡ do zbioru treningowego - tak moÅ¼esz zoverfitowaÄ‡ aplikacjÄ™ pod dany zbiÃ³r pytaÅ„ testowych, np. dla RAG wybierajÄ…c niewÅ‚aÅ›ciwy model embeddingÃ³w lub za bardzo naginajÄ…c prompt pod konkretne przypadki.

Polecam zadbaÄ‡ o duÅ¼y rozmiar zbioru pytaÅ„ testowych. A o ile masz na to zasoby - stwÃ³rz drugi zbiÃ³r pytaÅ„, ktÃ³ry bÄ™dziesz uruchamiaÄ‡ rzadziej, aby okazjonalnie weryfikowaÄ‡, czy nie doszÅ‚o do regresji :) Metodyka jest doÅ›Ä‡ zbliÅ¼ona do trenowania modeli ML.
  
## SÅ‚owa koÅ„cowe i odnoÅ›niki

Mam nadziejÄ™, Å¼e post okazaÅ‚ siÄ™ pomocny :) JeÅ›li chcesz przeczytaÄ‡ wiÄ™cej artykuÅ‚Ã³w w tej tematyce, zostawiam Ci listÄ™ w kontekÅ›cie RAG:
- [Systematically Improving Your RAG, Jason Liu](https://jxnl.co/writing/2024/05/22/systematically-improving-your-rag/) - o metodyce ulepszania RAG
- [RAG Evaluation: Donâ€™t let customers tell you first, Pinecone](https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/rag-evaluation/) - przedstawienie i wyjaÅ›nienie czÄ™stych metryk ewaluacyjnych

JeÅ›li zauwaÅ¼yÅ‚\*Å› jakieÅ› bÅ‚Ä™dy, nieÅ›cisÅ‚oÅ›ci, lub chciaÅ‚\*byÅ› po prostu porozmawiaÄ‡ o tej tematyce - zapraszam do kontaktu na [LinkedInie](https://www.linkedin.com/in/maciej-kaszkowiak/) lub e-mailowego - kontakt w stopce :)

{{< notice-feather-icons info >}}
JeÅ›li jesteÅ› doÅ›wiadczon\* w tematyce AI, mogÅ‚a rzuciÄ‡ Ci siÄ™ w oczy nieprecyzyjna nomenklatura! MÃ³wiÄ…c o AI, chodziÅ‚o mi wyÅ‚Ä…cznie o modele ML, w szczegÃ³lnoÅ›ci duÅ¼e modele jÄ™zykowe (LLM). Termin AI siÄ™ niestety nieprecyzyjnie przyjÄ…Å‚ w szerokiej Å›wiadomoÅ›ci i zostaÅ‚ utoÅ¼samiony z chatbotami i LLM. 

Uproszczona nomenklatura pozwala mi za to **dotrzeÄ‡ do szerszej grupy odbiorcÃ³w** - moÅ¼e wÅ‚aÅ›nie czyta to osoba, ktÃ³ra jeszcze nie miaÅ‚a okazji poznaÄ‡ rÃ³Å¼nicy pomiÄ™dzy AI/ML/LLM, ale po tej adnotacji wygoogluje akronimy :)

{{</ notice-feather-icons >}}

